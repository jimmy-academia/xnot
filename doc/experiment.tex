\section{Experiment}

\subsection{Setup}

\paragraph{Data.}
We construct our evaluation dataset from the Yelp Open Dataset, selecting 20 restaurants from a single metropolitan area based on review density.
For each restaurant, we sample 20 reviews stratified by star rating (4 reviews per rating level) to ensure balanced sentiment coverage.
Reviews are selected by descending text length within each stratum to maximize available information for reasoning.

\paragraph{User Requests.}
We design 20 natural-language user requests organized into four groups that test different reasoning capabilities:
\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{G01 (R00--R04):} Simple metadata conditions using direct item attributes (e.g., WiFi availability, noise level, parking).
    \item \textbf{G02 (R05--R09):} Mixed conditions combining structured metadata with unstructured review text analysis.
    \item \textbf{G03 (R10--R14):} Computed metadata requiring interpretation of business hours against user-specified time windows.
    \item \textbf{G04 (R15--R19):} Social metadata using reviewer credibility signals (elite status, review recency, community engagement).
\end{itemize}

\paragraph{Ground Truth.}
Each (restaurant, request) pair receives a ground truth label using three-valued logic: $+1$ (recommend), $0$ (unknown), or $-1$ (not recommend).
For structured metadata, we evaluate conditions directly against item attributes.
For review-based conditions, we employ LLM-based judgment with \emph{consensus voting}: a condition is satisfied only when at least 75\% of opinionated reviews agree.
Multiple conditions are aggregated using Boolean AND semantics where false dominates unknown, which dominates true.

\paragraph{Evaluation Metric.}
For each user request, we evaluate a method's ability to identify the correct restaurant using \emph{Hits@k}.
Each request has exactly one ground-truth positive item ($+1$ label).
The method produces a ranking of all 20 restaurants; Hits@k equals 1 if the ground-truth item appears in the top-$k$ positions, 0 otherwise.
We report Hits@1 as our primary metric, measuring whether the method's top recommendation matches the gold item.

\paragraph{Baselines.}
We compare against \textbf{Chain-of-Thought (CoT)}, a zero-shot prompting baseline that directly asks the LLM to evaluate each restaurant against the user request and select the best match from the candidate set.
