# General ANoT: Design Documentation

## Overview

General ANoT (Adaptive Network of Thought) is a task-agnostic evaluation framework that transforms a task formula into executable computations over restaurant reviews.

**Key Insight**: The system separates *what to compute* (derived from the task formula) from *how to execute* (applied to each restaurant's context).

---

## Architecture: Two Phases

```
┌─────────────────────────────────────────────────────────────┐
│ PHASE 1: COMPILE                                            │
│                                                             │
│   Input:  Task Formula (natural language specification)    │
│   Output: Formula Seed (structured JSON specification)      │
│                                                             │
│   Runs: ONCE per task                                       │
│   Uses: LLM to parse and structure the formula              │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│ PHASE 2: EXECUTE                                            │
│                                                             │
│   Input:  Formula Seed + Context (1 restaurant + reviews)   │
│   Output: Computed Results (primitives + verdict)           │
│                                                             │
│   Runs: ONCE per restaurant                                 │
│   Uses: LLM for extraction, Python for computation          │
└─────────────────────────────────────────────────────────────┘
```

### Why Two Phases?

| Concern | Phase 1 | Phase 2 |
|---------|---------|---------|
| Focus | Task/Query | Context/Data |
| Runs | Once per task | Once per restaurant |
| Cacheable | Yes (reuse across restaurants) | No (data-dependent) |
| LLM Usage | Parse formula | Extract from reviews |

### Why Not Three Phases?

Originally considered:
- Phase 1: Parse formula
- Phase 2: Adapt to context
- Phase 3: Execute computations

**Decision**: Merged Phase 2 and 3 because "adaptation" and "execution" are tightly coupled. Phase 2 both prepares data AND computes results.

---

## Formula Seed: The Phase 1 Output

The Formula Seed is a structured JSON specification that tells Phase 2 exactly what to do.

```json
{
  "task_name": "G1a",

  "filter_keywords": ["peanut", "allergy", "nut", ...],

  "extraction_fields": [
    {"name": "incident_severity", "type": "enum", "values": ["none", "mild", "moderate", "severe"]},
    {"name": "account_type", "type": "enum", "values": ["none", "firsthand", "secondhand", "hypothetical"]},
    {"name": "safety_interaction", "type": "enum", "values": ["none", "positive", "negative", "betrayal"]}
  ],

  "extraction_prompt": "Analyze this review:\n{review_text}\n...",

  "compute_dag": [
    {"name": "n_mild", "formula": "sum(1 for e in extractions if e['incident_severity']=='mild' and e['account_type']=='firsthand')"},
    {"name": "n_severe", "formula": "sum(1 for e in extractions if e['incident_severity']=='severe' and e['account_type']=='firsthand')"},
    {"name": "n_total", "formula": "n_mild + n_moderate + n_severe"},
    {"name": "incident_score", "formula": "n_mild * 2 + n_moderate * 5 + n_severe * 15"},
    {"name": "final_risk_score", "formula": "max(0.0, min(20.0, raw_risk))"},
    {"name": "verdict", "formula": "'Low Risk' if final_risk_score < 4 else ('High Risk' if final_risk_score < 8 else 'Critical Risk')"}
  ],

  "output_fields": ["n_total_incidents", "incident_score", "final_risk_score", "verdict"]
}
```

### Schema Contract

The Formula Seed uses **fixed keys** defined in code (not by LLM):
- `filter_keywords`
- `extraction_fields`
- `extraction_prompt`
- `compute_dag`
- `output_fields`

The **values** are generated by the Phase 1 LLM based on the task formula.

### Placeholder Variables

The extraction prompt uses fixed placeholder names:
- `{review_text}` - The review content
- `{review_date}` - The review date
- `{review_stars}` - The star rating
- `{review_useful}` - The useful votes count

These are hardcoded in Phase 1 and substituted by Phase 2.

---

## Phase 2 Execution: Three Steps

### Step 1: FILTER

```python
relevant_reviews = []
for review in context.reviews:
    if any(keyword in review.text.lower() for keyword in formula_seed.filter_keywords):
        relevant_reviews.append(review)
```

- **Executor**: Python (no LLM)
- **Purpose**: Find reviews that might contain relevant information
- **Output**: Subset of reviews to process

### Step 2: LLM_FOREACH (Extract)

```python
extractions = []
for review in relevant_reviews:
    prompt = formula_seed.extraction_prompt
    prompt = prompt.replace("{review_text}", review.text)
    prompt = prompt.replace("{review_stars}", str(review.stars))
    # ... other substitutions

    response = LLM(prompt)
    extraction = parse_json(response)
    extractions.append(extraction)
```

- **Executor**: LLM (one call per relevant review)
- **Purpose**: Extract structured signals from unstructured text
- **Output**: List of extraction dicts (one per review)
- **Scaling**: This is where N reviews → N LLM calls

### Step 3: COMPUTE (Execute Compute DAG)

```python
values = {"extractions": extractions}

for step in formula_seed.compute_dag:
    result = python_eval(step.formula, values)
    values[step.name] = result  # Store for later steps

return {field: values[field] for field in formula_seed.output_fields}
```

- **Executor**: Python `eval()` (no LLM)
- **Purpose**: Aggregate extractions and derive final values
- **Output**: Final computed primitives

---

## Compute DAG: The Derivation Structure

The Compute DAG is an ordered list of **Compute Steps** where:
1. Each step has a `name` (identifier)
2. Each step has a `formula` (Python expression)
3. Each step's result is stored and available to later steps
4. Steps must be in dependency order

### Example Compute DAG

```
extractions (input)
     │
     ├───────────────┬───────────────┐
     ▼               ▼               ▼
  n_mild         n_moderate       n_severe
     │               │               │
     └───────────────┼───────────────┘
                     ▼
                 n_total
                     │
     ┌───────────────┴───────────────┐
     ▼                               ▼
incident_score                 recency_decay
     │                               │
     └───────────────┬───────────────┘
                     ▼
              incident_impact
                     │
                     ▼
                 raw_risk
                     │
                     ▼
            final_risk_score
                     │
                     ▼
                 verdict
```

### Compute Step Execution

```python
# Step 1: n_mild
values["n_mild"] = eval("sum(1 for e in extractions if ...)", values)
# values = {extractions: [...], n_mild: 2}

# Step 2: n_moderate
values["n_moderate"] = eval("sum(1 for e in extractions if ...)", values)
# values = {extractions: [...], n_mild: 2, n_moderate: 1}

# Step 3: n_total (references previous steps)
values["n_total"] = eval("n_mild + n_moderate + n_severe", values)
# values = {..., n_total: 4}

# ... continues in order
```

### What Python `eval()` Can Handle

| Works | Doesn't Work |
|-------|--------------|
| `a + b * c` | `a = b` (assignment) |
| `'Low' if x < 4 else 'High'` | `if x < 4: return 'Low'` |
| `sum(1 for e in list if cond)` | `for e in list:` |
| `max(a, b)`, `len(list)` | `def f(x):` |
| `a and b or c` | `import x` |

All formulas must be **single expressions**, not statements.

---

## Key Design Decisions

### 1. JSON-Based Specification (Not Text Script)

**Decision**: Use structured JSON instead of text-based LWT script format.

**Reason**:
- Handles dynamic loops naturally (extraction count unknown until runtime)
- Easy to serialize/validate
- Clear separation of components

### 2. Python Execution for Compute (Not LLM)

**Decision**: Compute DAG formulas are executed by Python `eval()`, not sent to LLM.

**Reason**:
- Deterministic results
- Fast execution (no API calls)
- Precise arithmetic (no hallucination)

LLM is only used for EXTRACT step (understanding natural language reviews).

### 3. Hardcoded Schema Keys

**Decision**: Field names like `filter_keywords`, `extraction_fields` are hardcoded in code.

**Reason**:
- Ensures Phase 1 and Phase 2 agree on structure
- LLM fills in values, not keys
- Schema is a code contract, not LLM-generated

### 4. Merged Aggregate + Compute

**Decision**: No separate AGGREGATE step; aggregations are just Compute Steps that reference `extractions`.

**Reason**:
- Simpler architecture
- Aggregations and derivations are both just formulas
- Same execution mechanism for both

### 5. `extractions` as Implicit Variable

**Decision**: The `extractions` list is automatically available in Compute DAG.

**Reason**:
- Every task needs to reference extraction results
- Making it implicit simplifies the schema
- Well-documented contract between phases

---

## Terminology

| Term | Definition |
|------|------------|
| **Formula Seed** | The complete Phase 1 output; a JSON specification for evaluation |
| **Compute DAG** | The ordered list of computation steps within the Formula Seed |
| **Compute Step** | A single `{name, formula}` entry in the Compute DAG |
| **Extraction** | The structured output from LLM analyzing one review |
| **Primitive** | A computed value (e.g., `n_mild`, `incident_score`, `verdict`) |

### Naming in Papers

| Context | Format |
|---------|--------|
| First introduction | *formula seed* (italics) |
| Subsequent prose | Formula Seed (title case) |
| Code/figures | `FormulaSeed` (PascalCase) |

---

## File Structure

```
general_anot/
├── __init__.py           # Package exports
├── DESIGN.md             # This document
├── phase1_step1.py       # Extract conditions from formula
├── phase1_step2.py       # Build computation graph
├── phase1_step3.py       # Generate Formula Seed
├── phase2.py             # Execute Formula Seed
└── phase1.py             # (Legacy) Single-step Phase 1
```

---

## Future Considerations

### Adversarial Review Handling

Current design processes all filtered reviews. Could add:
```json
{
  "extraction_fields": [
    {"name": "review_validity", "values": ["valid", "spam", "adversarial"]}
  ]
}
```

Then Compute Steps filter: `sum(1 for e in extractions if e['review_validity']=='valid' and ...)`

### Long Review Handling

Current: Truncation hardcoded in Phase 2.

Could make explicit in Formula Seed:
```json
{
  "review_handling": {
    "max_length": 2000,
    "strategy": "truncate_head"
  }
}
```

### Multi-Pass Extraction

Current: Single extraction pass per review.

Could add validation/reconciliation step for edge cases.

---

## Summary

General ANoT transforms task formulas into executable evaluations through:

1. **Phase 1 (Compile)**: LLM parses formula → Formula Seed (JSON spec)
2. **Phase 2 (Execute)**:
   - FILTER: Find relevant reviews (Python)
   - LLM_FOREACH: Extract signals per review (LLM)
   - COMPUTE: Execute Compute DAG (Python)

The Compute DAG enables building complex primitives from simpler ones, with each Compute Step storing its result for use by later steps.
